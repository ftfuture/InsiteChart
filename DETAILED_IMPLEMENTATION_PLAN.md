# InsiteChart ìƒì„¸ êµ¬í˜„ ê³„íšì„œ (Detailed Implementation Plan)

**ì‘ì„±ì¼**: 2025ë…„ 12ì›” 11ì¼
**ë²„ì „**: 1.0
**ìƒíƒœ**: ì§„í–‰ ì¤‘
**ì´ ì‘ì—… í•­ëª©**: 62ê°œ

---

## ğŸ“‹ ëª©ì°¨

1. [ê°œìš”](#ê°œìš”)
2. [PRIORITY 1 - ì¦‰ì‹œ í•„ìˆ˜ ì™„ë£Œ](#priority-1---ì¦‰ì‹œ-í•„ìˆ˜-ì™„ë£Œ-5ê°œ-phase)
3. [PRIORITY 2 - ì¤‘ê¸° ì™„ë£Œ](#priority-2---ì¤‘ê¸°-ì™„ë£Œ-3-4ê°œì›”)
4. [PRIORITY 3 - ì¥ê¸° ê³ ë„í™”](#priority-3---ì¥ê¸°-ê³ ë„í™”-5-8ê°œì›”-ì´ìƒ)
5. [ì‘ì—… ì˜ì¡´ì„±](#ì‘ì—…-ì˜ì¡´ì„±)
6. [ìœ„í—˜ ë¶„ì„](#ìœ„í—˜-ë¶„ì„)
7. [ì„±ê³µ ê¸°ì¤€](#ì„±ê³µ-ê¸°ì¤€)

---

## ê°œìš”

InsiteChartëŠ” í˜„ì¬ **75-85% ì™„ì„±ë„**ì˜ ê¸ˆìœµ ë¶„ì„ í”Œë«í¼ì…ë‹ˆë‹¤.

**í˜„ì¬ ìƒíƒœ**:
- âœ… ì½”ì–´ ê¸°ëŠ¥ êµ¬í˜„ë¨ (ì£¼ì‹ ë°ì´í„°, ê¸°ë³¸ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„)
- âš ï¸ ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™” ë¶€ë¶„ êµ¬í˜„ (WebSocket ìˆì§€ë§Œ ì‹¤ì œ ì‘ë™ ì•ˆ í•¨)
- âŒ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬ ë¯¸êµ¬í˜„ (ì—¬ì „íˆ ëª¨ë†€ë¦¬ì‹)
- âŒ Kafka ë©”ì‹œì§€ í í…ŒìŠ¤íŠ¸ í™˜ê²½ë§Œ êµ¬ì„±
- âŒ GDPR ìë™í™” ê¸°ë³¸ êµ¬ì¡°ë§Œ êµ¬í˜„

ë³¸ ê³„íšì€ 62ê°œì˜ êµ¬ì²´ì ì¸ ì‘ì—…ì„ 3ê°œ ìš°ì„ ìˆœìœ„ë¡œ ë¶„ë¥˜í•˜ì—¬ ë‹¨ê³„ì ìœ¼ë¡œ êµ¬í˜„í•˜ê¸° ìœ„í•œ ë¡œë“œë§µì…ë‹ˆë‹¤.

---

## PRIORITY 1 - ì¦‰ì‹œ í•„ìˆ˜ ì™„ë£Œ (4ê°œ Phase)

### ì™„ë£Œ ê¸°í•œ: 1-2ê°œì›”
### ëª©í‘œ: í”„ë¡œë•ì…˜ ë°°í¬ë¥¼ ìœ„í•œ í•µì‹¬ ì¸í”„ë¼ êµ¬ì¶•

---

## Phase 1: ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™” ì‹œìŠ¤í…œ êµ¬ì¶•

**ëª©í‘œ**: ì£¼ì‹ ë°ì´í„°ì™€ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ì„ 1ì´ˆ ì´ë‚´ ì§€ì—°ì‹œê°„ìœ¼ë¡œ ì‹¤ì‹œê°„ ì „ì†¡
**ê¸°í•œ**: 2ì£¼
**í˜„ì¬ ìƒíƒœ**: WebSocket êµ¬ì¡°ë§Œ ìˆê³  ì‹¤ì œ ë™ê¸°í™” ë¯¸êµ¬í˜„

### 1.1 WebSocket ì—°ê²° ì•ˆì •í™” ë° ì¬ì—°ê²° ë©”ì»¤ë‹ˆì¦˜

**ìƒì„¸ ìš”êµ¬ì‚¬í•­**:
- ìë™ ì¬ì—°ê²° ë¡œì§ (exponential backoff: 1s â†’ 2s â†’ 4s â†’ max 30s)
- ì—°ê²° ëŠê¹€ ì‹œ ìë™ ì¬êµ¬ë…
- í•˜íŠ¸ë¹„íŠ¸/í•‘-í ë©”ì»¤ë‹ˆì¦˜ (30ì´ˆ ê°„ê²©)
- ìµœëŒ€ ë™ì‹œ ì—°ê²°: 1000ê°œ
- ì—°ê²° íƒ€ì„ì•„ì›ƒ: 60ì´ˆ

**êµ¬í˜„ íŒŒì¼**:
- `/home/user/InsiteChart/backend/api/websocket_routes.py` (ìˆ˜ì •)
  - í˜„ì¬ ìƒíƒœ: ê¸°ë³¸ WebSocket êµ¬ì¡°ë§Œ ì¡´ì¬
  - í•„ìš”í•œ ì¶”ê°€: ì¬ì—°ê²° ë©”ì»¤ë‹ˆì¦˜, í•˜íŠ¸ë¹„íŠ¸

**ê¸°ìˆ  ìŠ¤íƒ**:
- WebSockets 11.0
- asyncio íƒ€ì´ë° ê´€ë¦¬
- Redis Pub/Sub (ë¸Œë¡œë“œìºìŠ¤íŠ¸)

**í…ŒìŠ¤íŠ¸**:
```bash
pytest tests/test_websocket_reconnection.py -v --markers websocket
# í…ŒìŠ¤íŠ¸ í•­ëª©:
# - ì •ìƒ ì—°ê²°/í•´ì œ
# - ê°•ì œ ì—°ê²° ëŠê¹€ í›„ ìë™ ì¬ì—°ê²°
# - í•˜íŠ¸ë¹„íŠ¸ íƒ€ì„ì•„ì›ƒ
# - ìµœëŒ€ ì—°ê²° ìˆ˜ ë„ë‹¬ ì‹œ ì²˜ë¦¬
# - ë©”ì‹œì§€ ì†ì‹¤ ì—†ìŒ
```

---

### 1.2 ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸

**ìƒì„¸ ìš”êµ¬ì‚¬í•­**:
- **ì£¼ê°€ ë°ì´í„°**: 5ì´ˆë§ˆë‹¤ Yahoo Financeì—ì„œ ìˆ˜ì§‘ â†’ WebSocket ì „ì†¡
- **ê±°ë˜ëŸ‰ ë°ì´í„°**: 1ë¶„ë§ˆë‹¤ ì§‘ê³„
- **ì„¼í‹°ë¨¼íŠ¸ ì—…ë°ì´íŠ¸**: 10ì´ˆë§ˆë‹¤ (Redisì—ì„œ ìºì‹œëœ ë°ì´í„°)
- ë°ì´í„° ì†ì‹¤ ë°©ì§€: ê° ì—…ë°ì´íŠ¸ì— ì‹œí€€ìŠ¤ ë²ˆí˜¸

**êµ¬í˜„ ë‹¨ê³„**:

1. **ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬** (`backend/services/realtime_data_collector.py`):
```python
async def collect_stock_data_periodic():
    """5ì´ˆë§ˆë‹¤ Yahoo Financeì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    while True:
        try:
            for symbol in watched_symbols:
                data = await fetch_yahoo_finance(symbol)
                await cache.set(f"stock:{symbol}", data, ttl=5)
                await publish_to_redis(f"stock_updates:{symbol}", data)
        except Exception as e:
            logger.error(f"Data collection error: {e}")
        await asyncio.sleep(5)

async def collect_sentiment_data_periodic():
    """10ì´ˆë§ˆë‹¤ ì„¼í‹°ë¨¼íŠ¸ ë°ì´í„° ì—…ë°ì´íŠ¸"""
    while True:
        try:
            for symbol in watched_symbols:
                sentiment = await cache.get(f"sentiment:{symbol}")
                await publish_to_redis(f"sentiment_updates:{symbol}", sentiment)
        except Exception as e:
            logger.error(f"Sentiment collection error: {e}")
        await asyncio.sleep(10)
```

2. **WebSocket ë¸Œë¡œë“œìºìŠ¤íŠ¸**:
```python
async def broadcast_stock_update(symbol: str, data: dict):
    """ëª¨ë“  êµ¬ë… í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì—…ë°ì´íŠ¸ ì „ì†¡"""
    key = f"stock_updates:{symbol}"
    await redis_pub_sub.publish(key, json.dumps({
        "type": "stock_update",
        "symbol": symbol,
        "data": data,
        "sequence": next_sequence(),
        "timestamp": datetime.utcnow().isoformat()
    }))
```

**êµ¬í˜„ íŒŒì¼**:
- `/home/user/InsiteChart/backend/services/realtime_data_collector.py` (í˜„ì¬ ë¹„í™œì„±í™”ë¨ - ì¬í™œì„±í™”)
- `/home/user/InsiteChart/backend/main.py` (ë¼ì¸ 104 - ë¹„í™œì„±í™” ì œê±°)

**í…ŒìŠ¤íŠ¸**:
```bash
pytest tests/test_realtime_streaming.py -v
# í…ŒìŠ¤íŠ¸ í•­ëª©:
# - 5ì´ˆ ê°„ê²© ë°ì´í„° ìˆ˜ì§‘
# - ë°ì´í„° ì •í™•ì„± (Yahoo Finance vs ìºì‹œ)
# - ì„¸í€€ìŠ¤ ë²ˆí˜¸ ì—°ì†ì„±
# - ë ˆì´í„´ì‹œ < 1ì´ˆ
# - ë°ì´í„° ì†ì‹¤ ì—†ìŒ
```

---

### 1.3 Redis Pub/Sub ì´ë²¤íŠ¸ ë¸Œë¡œë“œìºìŠ¤íŠ¸ ì‹œìŠ¤í…œ

**ìƒì„¸ ìš”êµ¬ì‚¬í•­**:
- ì—¬ëŸ¬ ì„œë²„ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ë©”ì‹œì§€ ë¸Œë¡œë“œìºìŠ¤íŠ¸
- ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥
- êµ¬ë…ì ê´€ë¦¬ (ìë™ ì •ë¦¬)
- ë©”ì‹œì§€ ì¬ì „ì†¡ ë¡œì§ (ì‹¤íŒ¨ ì‹œ)

**êµ¬í˜„**:

```python
# Redis Pub/Sub Manager
class RedisPubSubManager:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.subscriptions = {}  # channel â†’ [callback, ...]

    async def subscribe(self, channel: str, callback):
        """ì±„ë„ êµ¬ë…"""
        if channel not in self.subscriptions:
            self.subscriptions[channel] = []
        self.subscriptions[channel].append(callback)

    async def publish(self, channel: str, message: dict):
        """ë©”ì‹œì§€ ë°œí–‰"""
        serialized = json.dumps({
            **message,
            "sequence": self.get_next_sequence(),
            "timestamp": datetime.utcnow().isoformat(),
            "publisher": get_server_id()
        })
        await self.redis.publish(channel, serialized)

    async def listen(self, channel: str):
        """ì±„ë„ ë©”ì‹œì§€ ìˆ˜ì‹ """
        async with self.redis.pubsub() as pubsub:
            await pubsub.subscribe(channel)
            while True:
                message = await pubsub.get_message()
                if message and message['type'] == 'message':
                    for callback in self.subscriptions.get(channel, []):
                        await callback(message['data'])
```

**ì±„ë„ ë„¤ì´ë° ì»¨ë²¤ì…˜**:
- `stock_updates:{symbol}` - ì£¼ê°€ ì—…ë°ì´íŠ¸
- `sentiment_updates:{symbol}` - ì„¼í‹°ë¨¼íŠ¸ ì—…ë°ì´íŠ¸
- `alert:{user_id}` - ì‚¬ìš©ìë³„ ì•Œë¦¼
- `system:notifications` - ì‹œìŠ¤í…œ ì•Œë¦¼

**êµ¬í˜„ íŒŒì¼**:
- `/home/user/InsiteChart/backend/cache/redis_pubsub_manager.py` (ìƒˆ íŒŒì¼)

---

### 1.4 ë™ì‹œì„± ì²˜ë¦¬ ë° ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥

**ìƒì„¸ ìš”êµ¬ì‚¬í•­**:
- ë©€í‹° ìŠ¤ë ˆë“œ/í”„ë¡œì„¸ìŠ¤ í™˜ê²½ì—ì„œ ë©”ì‹œì§€ ìˆœì„œ ìœ ì§€
- ë°ì´í„° ê²½í•©(race condition) ë°©ì§€
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ (long-running async tasks)
- ë°ë“œë½(deadlock) ë°©ì§€

**êµ¬í˜„ ì „ëµ**:

```python
# ê¸€ë¡œë²Œ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ìƒì„±ê¸° (Redis ì‚¬ìš© - ë¶„ì‚° í™˜ê²½)
class SequenceGenerator:
    def __init__(self, redis_client):
        self.redis = redis_client

    async def next_sequence(self, key: str = "global_sequence") -> int:
        """ë¶„ì‚° í™˜ê²½ì—ì„œ ì•ˆì „í•œ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ìƒì„±"""
        # Redis INCRëŠ” atomic operation
        return await self.redis.incr(key)

# ë©”ì‹œì§€ í (ìˆœì„œ ë³´ì¥)
class OrderedMessageQueue:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.queue_key = "message_queue"

    async def enqueue(self, message: dict) -> int:
        """ë©”ì‹œì§€ ì¶”ê°€ (ìˆœì„œ ë³´ì¥)"""
        sequence = await SequenceGenerator(self.redis).next_sequence()
        message['_sequence'] = sequence
        await self.redis.rpush(self.queue_key, json.dumps(message))
        return sequence

    async def dequeue(self, timeout=1) -> dict:
        """ë©”ì‹œì§€ ì¶”ì¶œ (FIFO)"""
        data = await self.redis.blpop(self.queue_key, timeout=timeout)
        return json.loads(data) if data else None
```

**í…ŒìŠ¤íŠ¸**:
```bash
pytest tests/test_concurrent_messaging.py -v
# í…ŒìŠ¤íŠ¸ í•­ëª©:
# - 1000 ë™ì‹œ ì—°ê²°
# - ë©”ì‹œì§€ ìˆœì„œ ìœ ì§€ (ì‹œí€€ìŠ¤ ë²ˆí˜¸ë¡œ ê²€ì¦)
# - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ
# - ë°ë“œë½ ì—†ìŒ
```

---

### 1.5 ì‹¤ì‹œê°„ ì•Œë¦¼ í…œí”Œë¦¿ ë° ë‹¤êµ­ì–´ ì§€ì›

**ìƒì„¸ ìš”êµ¬ì‚¬í•­**:
- ì•Œë¦¼ í…œí”Œë¦¿ ì‹œìŠ¤í…œ (ë™ì  ë³€ìˆ˜ ì¹˜í™˜)
- 13ê°œ ì–¸ì–´ ì§€ì›
- ì•Œë¦¼ ìš°ì„ ìˆœìœ„ (CRITICAL, HIGH, MEDIUM, LOW)
- ì‚¬ìš©ìë³„ ì•Œë¦¼ ì„¤ì • ì €ì¥

**ì•Œë¦¼ í…œí”Œë¦¿ ì˜ˆì‹œ**:

```json
{
  "template_id": "price_alert",
  "name": "ê°€ê²© ì•Œë¦¼",
  "subject": "{stock_symbol} ê°€ê²© ë³€ë™",
  "templates": {
    "en": {
      "subject": "{stock_symbol} Price Alert",
      "body": "{stock_symbol} has moved {change_percent}% to {current_price}",
      "priority": "HIGH"
    },
    "ko": {
      "subject": "{stock_symbol} ê°€ê²© ì•Œë¦¼",
      "body": "{stock_symbol}ê°€ {change_percent}% ë³€ë™í•˜ì—¬ {current_price}ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤",
      "priority": "HIGH"
    }
  }
}
```

**êµ¬í˜„ íŒŒì¼**:
- `/home/user/InsiteChart/backend/services/notification_template_service.py` (í™•ì¥)
- `/home/user/InsiteChart/backend/models/database_models.py` (NotificationTemplate í…Œì´ë¸” ì¶”ê°€)

**ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ**:
```sql
CREATE TABLE notification_templates (
    id SERIAL PRIMARY KEY,
    template_id VARCHAR(100) UNIQUE,
    name VARCHAR(200),
    description TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE notification_translations (
    id SERIAL PRIMARY KEY,
    template_id INT REFERENCES notification_templates(id),
    language VARCHAR(10),  -- 'en', 'ko', etc.
    subject TEXT,
    body TEXT,
    priority VARCHAR(20),  -- 'CRITICAL', 'HIGH', etc.
    UNIQUE(template_id, language)
);

CREATE TABLE user_notification_settings (
    id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(id),
    template_id INT REFERENCES notification_templates(id),
    enabled BOOLEAN DEFAULT TRUE,
    preferred_language VARCHAR(10),
    channels TEXT[],  -- ['email', 'push', 'websocket']
    quiet_hours_start TIME,
    quiet_hours_end TIME,
    UNIQUE(user_id, template_id)
);
```

---

## Phase 2: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ ë¶„ë¦¬

**ëª©í‘œ**: ëª¨ë†€ë¦¬ì‹ ì•„í‚¤í…ì²˜ë¥¼ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬
**ê¸°í•œ**: 4ì£¼
**í˜„ì¬ ìƒíƒœ**: ì„œë¹„ìŠ¤ ë ˆì´ì–´ ì¡´ì¬í•˜ì§€ë§Œ ê°•í•˜ê²Œ ê²°í•©ë¨ (ëª¨ë†€ë¦¬ì‹)

### 2.1 ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ ë…ë¦½í™”

**ëª©í‘œ**: ì£¼ì‹ ë°ì´í„°, Reddit ì„¼í‹°ë¨¼íŠ¸, Twitter ì„¼í‹°ë¨¼íŠ¸ ìˆ˜ì§‘ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ

**ì„œë¹„ìŠ¤ êµ¬ì¡°**:

```
data-collector-service/
â”œâ”€ main.py (FastAPI ì•±)
â”œâ”€ collectors/
â”‚  â”œâ”€ yahoo_finance_collector.py
â”‚  â”œâ”€ reddit_collector.py
â”‚  â”œâ”€ twitter_collector.py
â”‚  â””â”€ news_collector.py
â”œâ”€ models/
â”‚  â””â”€ collector_models.py
â””â”€ requirements.txt
```

**API ì—”ë“œí¬ì¸íŠ¸**:
```
POST /api/v1/collect/stocks
Body: { "symbols": ["AAPL", "MSFT"], "priority": "HIGH" }
Response: { "job_id": "uuid", "status": "started" }

GET /api/v1/collect/status/{job_id}
Response: { "job_id": "uuid", "status": "running", "progress": 45 }

POST /api/v1/collect/sentiment
Body: { "symbols": ["AAPL"], "sources": ["reddit", "twitter"] }
```

**í†µì‹  ë°©ì‹**:
- ë™ê¸°: HTTP/REST API
- ë¹„ë™ê¸°: Kafka ë©”ì‹œì§€ í (Phase 4ì—ì„œ êµ¬í˜„)

**êµ¬í˜„ ì„¸ë¶€ì‚¬í•­**:

```python
# data-collector-service/main.py
from fastapi import FastAPI
from .collectors import YahooFinanceCollector, RedditCollector

app = FastAPI()
yahoo_collector = YahooFinanceCollector()
reddit_collector = RedditCollector()

@app.post("/api/v1/collect/stocks")
async def collect_stocks(symbols: List[str]):
    """ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘"""
    job_id = str(uuid4())

    # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ìœ¼ë¡œ ì‹¤í–‰
    asyncio.create_task(
        run_collection_job(job_id, symbols)
    )

    return {
        "job_id": job_id,
        "status": "started",
        "symbols": symbols
    }

async def run_collection_job(job_id: str, symbols: List[str]):
    """ì‹¤ì œ ìˆ˜ì§‘ ë¡œì§"""
    try:
        results = []
        for symbol in symbols:
            stock_data = await yahoo_collector.collect(symbol)
            await redis.set(f"stock:{symbol}", stock_data)

            # Kafkaë¡œ ì´ë²¤íŠ¸ ë°œí–‰ (Phase 4)
            # await kafka_producer.send("stock_updates", {
            #     "symbol": symbol,
            #     "data": stock_data,
            #     "timestamp": datetime.utcnow().isoformat()
            # })

            results.append(stock_data)

        # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
        await redis.set(f"job:{job_id}", {
            "status": "completed",
            "results_count": len(results),
            "timestamp": datetime.utcnow().isoformat()
        })
    except Exception as e:
        logger.error(f"Collection job {job_id} failed: {e}")
        await redis.set(f"job:{job_id}", {
            "status": "failed",
            "error": str(e)
        })
```

**Docker ë°°í¬**:
```dockerfile
# data-collector-service/Dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001"]
```

**Docker Compose ì¶”ê°€**:
```yaml
# docker-compose.ymlì— ì¶”ê°€
data-collector-service:
  build:
    context: ./services/data-collector
  ports:
    - "8001:8000"
  environment:
    - REDIS_URL=redis://redis:6379
    - KAFKA_BROKERS=kafka:9092
  depends_on:
    - redis
    - kafka
  networks:
    - insitechart-network
```

**í…ŒìŠ¤íŠ¸**:
```bash
# services/data-collector/tests/
pytest test_collectors.py -v --markers collector
# - Yahoo Finance ìˆ˜ì§‘ ì •í™•ì„±
# - Reddit ë°ì´í„° íŒŒì‹±
# - Twitter ë°ì´í„° íŒŒì‹±
# - ì—ëŸ¬ ì²˜ë¦¬
# - ì¬ì‹œë„ ë¡œì§
```

---

### 2.2 ë¶„ì„ ì„œë¹„ìŠ¤ ë¶„ë¦¬

**ëª©í‘œ**: ì„¼í‹°ë¨¼íŠ¸, ìƒê´€ê´€ê³„, íŠ¸ë Œë“œ ë¶„ì„ì„ ë³„ë„ ì„œë¹„ìŠ¤ë¡œ

**ì„œë¹„ìŠ¤ êµ¬ì¡°**:

```
analytics-service/
â”œâ”€ main.py
â”œâ”€ analyzers/
â”‚  â”œâ”€ sentiment_analyzer.py (VADER, BERT)
â”‚  â”œâ”€ correlation_analyzer.py
â”‚  â”œâ”€ trend_analyzer.py
â”‚  â””â”€ ml_models/
â”‚      â”œâ”€ bert_model.py
â”‚      â””â”€ ml_trend_model.py
â”œâ”€ models/
â”‚  â””â”€ analysis_models.py
â””â”€ requirements.txt
```

**API ì—”ë“œí¬ì¸íŠ¸**:
```
POST /api/v1/analyze/sentiment
Body: { "symbol": "AAPL", "sources": ["reddit", "twitter"] }
Response: {
  "symbol": "AAPL",
  "sentiment": {
    "compound": 0.65,
    "positive": 0.72,
    "negative": 0.15,
    "neutral": 0.13,
    "confidence": 0.92,
    "model": "bert"
  }
}

POST /api/v1/analyze/correlation
Body: { "symbols": ["AAPL", "MSFT", "GOOGL"], "period": "1mo" }
Response: {
  "correlation_matrix": [[1.0, 0.85, 0.78], ...],
  "strong_pairs": [{"symbol1": "AAPL", "symbol2": "MSFT", "coef": 0.85}]
}

POST /api/v1/analyze/trends
Body: { "symbol": "AAPL", "lookback_days": 30 }
Response: {
  "trend": "uptrend",
  "strength": 0.85,
  "support_levels": [150.0, 148.5],
  "resistance_levels": [165.0, 167.5],
  "anomalies": [{"timestamp": "...", "magnitude": 2.5}]
}
```

**êµ¬í˜„ ì„¸ë¶€ì‚¬í•­**:

```python
# analytics-service/analyzers/sentiment_analyzer.py
class SentimentAnalyzer:
    def __init__(self):
        self.vader = SentimentIntensityAnalyzer()
        self.bert_model = BertForSequenceClassification.from_pretrained(
            'distilbert-base-uncased-finetuned-sst-2-english'
        )

    async def analyze(self, symbol: str, text: str, model: str = "ensemble"):
        """ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ (VADER ë˜ëŠ” BERT)"""
        if model == "vader":
            return await self._analyze_vader(text)
        elif model == "bert":
            return await self._analyze_bert(text)
        else:  # ensemble
            vader_result = await self._analyze_vader(text)
            bert_result = await self._analyze_bert(text)

            # ì•™ìƒë¸”: ê°€ì¤‘ í‰ê· 
            return {
                "compound": (vader_result["compound"] + bert_result["score"]) / 2,
                "positive": max(vader_result["pos"], bert_result["positive"]),
                "negative": max(vader_result["neg"], bert_result["negative"]),
                "neutral": vader_result["neu"],
                "confidence": min(vader_result["confidence"], bert_result["confidence"]),
                "model": "ensemble"
            }
```

---

### 2.3 API ê²Œì´íŠ¸ì›¨ì´ êµ¬í˜„ ë° ë¼ìš°íŒ… ë¡œì§

**ëª©í‘œ**: í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ì„ ì˜¬ë°”ë¥¸ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë¡œ ë¼ìš°íŒ…

**ê²Œì´íŠ¸ì›¨ì´ êµ¬ì¡°**:

```
api-gateway/
â”œâ”€ main.py
â”œâ”€ router.py (ë¼ìš°íŒ… ê·œì¹™)
â”œâ”€ circuit_breaker.py (ì„œí‚· ë¸Œë ˆì´ì»¤)
â”œâ”€ rate_limiter.py (ë¶„ì‚° ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…)
â””â”€ service_registry.py (ì„œë¹„ìŠ¤ ë””ìŠ¤ì»¤ë²„ë¦¬)
```

**ë¼ìš°íŒ… ê·œì¹™ ì˜ˆì‹œ**:

```python
# api-gateway/router.py
ROUTE_MAP = {
    "/api/v1/stocks": {
        "service": "backend-api",
        "url": "http://backend-api:8000",
        "timeout": 30,
        "circuit_breaker": True,
        "rate_limit": {"requests": 100, "window": 60}
    },
    "/api/v1/collect": {
        "service": "data-collector",
        "url": "http://data-collector:8001",
        "timeout": 60,
        "circuit_breaker": True,
        "rate_limit": {"requests": 50, "window": 60}
    },
    "/api/v1/analyze": {
        "service": "analytics",
        "url": "http://analytics:8002",
        "timeout": 30,
        "circuit_breaker": True,
        "rate_limit": {"requests": 100, "window": 60}
    }
}

# api-gateway/main.py
class APIGateway(FastAPI):
    async def route_request(self, path: str, method: str, body: dict):
        """ìš”ì²­ ë¼ìš°íŒ…"""
        route_config = self._find_route(path)

        # ì„œí‚· ë¸Œë ˆì´ì»¤ ì²´í¬
        if not self.circuit_breaker.is_healthy(route_config["service"]):
            return {"error": "Service temporarily unavailable"}

        # ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ì²´í¬
        if not self.rate_limiter.allow_request(path):
            return {"error": "Rate limit exceeded"}, 429

        # ë°±ì—”ë“œ ì„œë¹„ìŠ¤ë¡œ í”„ë¡ì‹œ
        try:
            response = await self._proxy_request(
                route_config["url"],
                path,
                method,
                body,
                timeout=route_config["timeout"]
            )
            return response
        except asyncio.TimeoutError:
            self.circuit_breaker.record_failure(route_config["service"])
            return {"error": "Service timeout"}, 504
        except Exception as e:
            logger.error(f"Routing error: {e}")
            return {"error": "Internal server error"}, 500
```

**ì„œí‚· ë¸Œë ˆì´ì»¤ êµ¬í˜„**:

```python
# api-gateway/circuit_breaker.py
class CircuitBreaker:
    def __init__(self, failure_threshold=5, recovery_timeout=60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.services = {}  # service_name â†’ {failures, last_failure, state}

    def is_healthy(self, service_name: str) -> bool:
        """ì„œë¹„ìŠ¤ ê±´ê°• ìƒíƒœ í™•ì¸"""
        if service_name not in self.services:
            return True

        service = self.services[service_name]

        # OPEN ìƒíƒœ (circuit breaker ì—´ë¦¼)
        if service["failures"] >= self.failure_threshold:
            # ë³µêµ¬ íƒ€ì„ì•„ì›ƒ ê²½ê³¼í–ˆëŠ”ì§€ í™•ì¸
            if time.time() - service["last_failure"] > self.recovery_timeout:
                service["failures"] = 0  # ë¦¬ì…‹
                return True
            return False

        return True

    def record_failure(self, service_name: str):
        """ì‹¤íŒ¨ ê¸°ë¡"""
        if service_name not in self.services:
            self.services[service_name] = {"failures": 0, "last_failure": 0, "state": "CLOSED"}

        self.services[service_name]["failures"] += 1
        self.services[service_name]["last_failure"] = time.time()

    def record_success(self, service_name: str):
        """ì„±ê³µ ê¸°ë¡"""
        if service_name in self.services:
            self.services[service_name]["failures"] = 0
```

**ë¶„ì‚° ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…** (Redis ì‚¬ìš©):

```python
# api-gateway/rate_limiter.py
class RedisRateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client

    async def allow_request(self, endpoint: str, user_id: str = "anonymous") -> bool:
        """ìš”ì²­ í—ˆìš© ì—¬ë¶€ í™•ì¸"""
        key = f"rate_limit:{endpoint}:{user_id}"

        # í˜„ì¬ ìš”ì²­ ìˆ˜ í™•ì¸
        current = await self.redis.get(key)
        current_count = int(current) if current else 0

        # ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ì„¤ì • ì¡°íšŒ
        limit = RATE_LIMITS.get(endpoint, {})
        max_requests = limit.get("requests", 100)
        window = limit.get("window", 60)

        if current_count >= max_requests:
            return False

        # ì¹´ìš´íŠ¸ ì¦ê°€
        pipe = self.redis.pipeline()
        pipe.incr(key)
        pipe.expire(key, window)
        await pipe.execute()

        return True
```

**êµ¬í˜„ íŒŒì¼**:
- `/home/user/InsiteChart/backend/api/gateway.py` (í™•ì¥)

---

### 2.4 ì„œë¹„ìŠ¤ ê°„ í†µì‹  ë° ë©”ì‹œì§€ í (Kafka ì¤€ë¹„)

**ëª©í‘œ**: ì„œë¹„ìŠ¤ ê°„ ëŠìŠ¨í•œ ê²°í•©(loose coupling) êµ¬í˜„

**í†µì‹  íŒ¨í„´**:

```
ë™ê¸° í†µì‹  (HTTP REST):
í´ë¼ì´ì–¸íŠ¸ â†’ API Gateway â†’ íŠ¹ì • ì„œë¹„ìŠ¤ â†’ ì‘ë‹µ

ë¹„ë™ê¸° í†µì‹  (Kafka):
Service A â†’ Kafka Topic â†’ Service B (ìˆ˜ì‹ )
Service A â†’ Kafka Topic â†’ Service C (ìˆ˜ì‹ )
```

**Kafka í† í”½ ì„¤ê³„**:

```
Topics:
â”œâ”€ stock_updates (stock-collector â†’ analytics, backend)
â”‚  â””â”€ Partitions: 5 (symbol ê¸°ë°˜)
â”œâ”€ sentiment_updates (sentiment-analyzer â†’ backend)
â”‚  â””â”€ Partitions: 5
â”œâ”€ alerts (backend â†’ notification-service)
â”‚  â””â”€ Partitions: 3 (user_id ê¸°ë°˜)
â”œâ”€ user_events (backend â†’ analytics)
â”‚  â””â”€ Partitions: 5
â””â”€ system_events (all â†’ monitoring)
   â””â”€ Partitions: 1
```

**ë©”ì‹œì§€ í¬ë§·**:

```json
{
  "event_type": "stock_update",
  "event_id": "uuid",
  "source_service": "data-collector",
  "timestamp": "2025-12-11T10:30:45.123Z",
  "data": {
    "symbol": "AAPL",
    "price": 150.25,
    "volume": 1000000,
    "change_percent": 1.5
  },
  "version": "1.0",
  "correlation_id": "uuid"  // ìš”ì²­ ì¶”ì ìš©
}
```

**êµ¬í˜„** (Phase 4ì—ì„œ ìƒì„¸ êµ¬í˜„):
- í˜„ì¬: Kafka í† í”½ ì„¤ê³„ë§Œ ì§„í–‰
- Phase 4: ì‹¤ì œ í”„ë¡œë“€ì„œ/ì»¨ìŠˆë¨¸ êµ¬í˜„

---

### 2.5 ê° ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì˜ ë…ë¦½ì  ë°°í¬ ë° ìŠ¤ì¼€ì¼ë§ ì„¤ì •

**ëª©í‘œ**: ê° ì„œë¹„ìŠ¤ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ë°°í¬, ì—…ë°ì´íŠ¸, ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥í•˜ê²Œ

**Docker Compose êµ¬ì¡°** (Phase 1 ì™„ë£Œ í›„):

```yaml
# í–¥í›„ docker-compose.prod.yml
version: '3.8'

services:
  # ê¸°ì¡´ ë°±ì—”ë“œ (ì¶•ì†Œë¨)
  backend-api:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://...
      - REDIS_URL=redis://redis:6379
      - KAFKA_BROKERS=kafka:9092
    depends_on:
      - postgres
      - redis
      - kafka

  # ìƒˆë¡œìš´ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤
  data-collector:
    build: ./services/data-collector
    ports:
      - "8001:8000"
    environment:
      - KAFKA_BROKERS=kafka:9092
    deploy:
      replicas: 3  # 3ê°œ ì¸ìŠ¤í„´ìŠ¤
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  analytics-service:
    build: ./services/analytics
    ports:
      - "8002:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - KAFKA_BROKERS=kafka:9092
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '1'  # BERT ëª¨ë¸ ë•Œë¬¸ì— ë†’ìŒ
          memory: 1G

  api-gateway:
    build: ./services/api-gateway
    ports:
      - "8080:8000"
    environment:
      - BACKEND_URL=http://backend-api:8000
      - DATA_COLLECTOR_URL=http://data-collector:8000
      - ANALYTICS_URL=http://analytics-service:8000
    depends_on:
      - backend-api
      - data-collector
      - analytics-service

  # ê¸°ì¡´ ì¸í”„ë¼ (postgres, redis, kafka, nginx ë“±)
  postgres:
    image: postgres:15-alpine
    ...
```

**ì„œë¹„ìŠ¤ë³„ ìŠ¤ì¼€ì¼ë§ ì „ëµ**:

```
data-collector:
  - ìˆ˜í‰ í™•ì¥ (Horizontal Scaling)
  - ì¸ìŠ¤í„´ìŠ¤ 3ê°œ â†’ 10ê°œë¡œ í™•ì¥
  - Kafka íŒŒí‹°ì…˜ ìˆ˜ì™€ ì¼ì¹˜ (5ê°œ)
  - ë¶€í•˜: CPU ê¸°ë°˜

analytics-service:
  - ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥
  - BERT ëª¨ë¸ ë•Œë¬¸ì— ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
  - ì¸ìŠ¤í„´ìŠ¤ë‹¹ 1GB ì´ìƒ í•„ìš”
  - ë¶€í•˜: CPU, GPU (ìˆìœ¼ë©´) ê¸°ë°˜

backend-api:
  - ìƒíƒœ ìˆìŒ (ì„¸ì…˜, ì»¨í…ìŠ¤íŠ¸) â†’ ìˆ˜ì§ í™•ì¥ ê¶Œì¥
  - Sticky session í•„ìš”
  - ì¸ìŠ¤í„´ìŠ¤ 1ê°œ â†’ 2-3ê°œë¡œ í™•ì¥
  - ë¶€í•˜: ìš”ì²­ ìˆ˜ ê¸°ë°˜
```

**ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­** (Prometheus):

```yaml
# monitoring/prometheus.yml ì¶”ê°€
scrape_configs:
  - job_name: 'data-collector'
    static_configs:
      - targets: ['localhost:8001', 'localhost:8001', 'localhost:8001']

  - job_name: 'analytics-service'
    static_configs:
      - targets: ['localhost:8002', 'localhost:8002']
```

**ë°°í¬ íŒŒì¼** (kubernetes ì¤€ë¹„):

```yaml
# k8s/data-collector-deployment.yaml (í–¥í›„)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-collector
spec:
  replicas: 3
  selector:
    matchLabels:
      app: data-collector
  template:
    metadata:
      labels:
        app: data-collector
    spec:
      containers:
      - name: data-collector
        image: insitechart/data-collector:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
```

---

## Phase 4: Kafka ì´ë²¤íŠ¸ ë²„ìŠ¤ ì‹¤ì œ í†µí•©

**ëª©í‘œ**: í…ŒìŠ¤íŠ¸ í™˜ê²½ì˜ Kafkaë¥¼ í”„ë¡œë•ì…˜ í™˜ê²½ìœ¼ë¡œ ì´ë™ ë° ì™„ì „ í†µí•©
**ê¸°í•œ**: 3ì£¼
**í˜„ì¬ ìƒíƒœ**: ì½”ë“œëŠ” êµ¬í˜„ë˜ì—ˆì§€ë§Œ í…ŒìŠ¤íŠ¸ í™˜ê²½(`docker-compose.test.yml`)ì—ë§Œ ì¡´ì¬

### 4.1 Kafka í´ëŸ¬ìŠ¤í„° ì„¤ì • ë° í† í”½ êµ¬ì„±

**í”„ë¡œë•ì…˜ Kafka êµ¬ì„±**: `docker-compose.yml` ìˆ˜ì •

```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - insitechart-network

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"  # ë‚´ë¶€ í¬íŠ¸
      - "9092:9092"  # ì™¸ë¶€ í¬íŠ¸ (í´ë¼ì´ì–¸íŠ¸ìš©)
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_INTERNAL://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1GB
    networks:
      - insitechart-network

  kafka-ui:  # Kafka ëª¨ë‹ˆí„°ë§ UI
    image: provectuslabs/kafka-ui:latest
    depends_on:
      - kafka
    ports:
      - "8082:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: insitechart
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - insitechart-network
```

**í† í”½ ìƒì„± ìŠ¤í¬ë¦½íŠ¸**: `scripts/create-kafka-topics.sh`

```bash
#!/bin/bash

# Kafkaê°€ ì‹œì‘ë  ë•Œê¹Œì§€ ëŒ€ê¸°
until docker-compose exec kafka kafka-topics --bootstrap-server kafka:9092 --list > /dev/null 2>&1; do
  echo "Waiting for Kafka..."
  sleep 2
done

# í† í”½ ìƒì„±
KAFKA_CMD="docker-compose exec -T kafka kafka-topics --bootstrap-server kafka:9092 --create"

# 1. stock_updates í† í”½
$KAFKA_CMD --topic stock_updates \
  --partitions 5 \
  --replication-factor 1 \
  --config retention.ms=86400000 \  # 24ì‹œê°„
  --config compression.type=snappy || echo "stock_updates already exists"

# 2. sentiment_updates í† í”½
$KAFKA_CMD --topic sentiment_updates \
  --partitions 5 \
  --replication-factor 1 \
  --config retention.ms=86400000 || echo "sentiment_updates already exists"

# 3. alerts í† í”½
$KAFKA_CMD --topic alerts \
  --partitions 3 \
  --replication-factor 1 \
  --config retention.ms=604800000 \ # 7ì¼
  --config compression.type=snappy || echo "alerts already exists"

# 4. user_events í† í”½
$KAFKA_CMD --topic user_events \
  --partitions 5 \
  --replication-factor 1 \
  --config retention.ms=604800000 || echo "user_events already exists"

# 5. system_events í† í”½
$KAFKA_CMD --topic system_events \
  --partitions 1 \
  --replication-factor 1 \
  --config retention.ms=2592000000 \ # 30ì¼
  --config compression.type=gzip || echo "system_events already exists"

echo "Kafka topics created successfully!"
```

**í† í”½ë³„ íŒŒí‹°ì…˜ ì„¤ê³„**:

```
stock_updates (5 partitions):
  - symbol ê¸°ë°˜ íŒŒí‹°ì…”ë‹
  - ê° íŒŒí‹°ì…˜ì€ íŠ¹ì • ì‹¬ë³¼ ê·¸ë£¹ì„ ë‹´ë‹¹
  - ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥

sentiment_updates (5 partitions):
  - source ê¸°ë°˜ íŒŒí‹°ì…”ë‹
  - Reddit, Twitter, Discord ë“± ê° ì†ŒìŠ¤ë³„ ì²˜ë¦¬

alerts (3 partitions):
  - user_id % 3 ìœ¼ë¡œ íŒŒí‹°ì…”ë‹
  - ì‚¬ìš©ìë³„ ì•Œë¦¼ ìˆœì„œ ë³´ì¥

user_events (5 partitions):
  - event_type ê¸°ë°˜
  - ì´ë²¤íŠ¸ íƒ€ì…ë³„ ë¶„ì„

system_events (1 partition):
  - ìˆœì„œ ë³´ì¥ í•„ìˆ˜
  - íŒŒí‹°ì…˜ 1ê°œë§Œ ì‚¬ìš©
```

---

### 4.2 Kafka í”„ë¡œë“€ì„œ êµ¬í˜„

**íŒŒì¼**: `backend/services/kafka_event_producer.py`

```python
from aiokafka import AIOKafkaProducer
import json
from typing import Dict, Any
from datetime import datetime
import uuid

class KafkaEventProducer:
    def __init__(self, bootstrap_servers="kafka:9092"):
        self.bootstrap_servers = bootstrap_servers
        self.producer = None

    async def start(self):
        """í”„ë¡œë“€ì„œ ì‹œì‘"""
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            compression_type='snappy',
            acks='all',  # ëª¨ë“  ë³µì œë³¸ì—ì„œ í™•ì¸
            retries=3,
            linger_ms=10,  # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•´ 10ms ëŒ€ê¸°
        )
        await self.producer.start()

    async def stop(self):
        """í”„ë¡œë“€ì„œ ì¢…ë£Œ"""
        if self.producer:
            await self.producer.stop()

    async def send_stock_update(self, symbol: str, price: float, volume: int):
        """ì£¼ê°€ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸"""
        event = {
            "event_type": "stock_update",
            "event_id": str(uuid.uuid4()),
            "source_service": "backend-api",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "data": {
                "symbol": symbol,
                "price": price,
                "volume": volume,
                "currency": "USD"
            },
            "version": "1.0"
        }

        # symbol ê¸°ë°˜ íŒŒí‹°ì…”ë‹ì„ ìœ„í•´ keyë¡œ symbol ì‚¬ìš©
        await self.producer.send_and_wait(
            "stock_updates",
            value=json.dumps(event).encode(),
            key=symbol.encode(),
            timestamp_ms=int(datetime.utcnow().timestamp() * 1000)
        )

    async def send_sentiment_update(self, symbol: str, sentiment: Dict[str, float]):
        """ì„¼í‹°ë¨¼íŠ¸ ì—…ë°ì´íŠ¸ ì´ë²¤íŠ¸"""
        event = {
            "event_type": "sentiment_update",
            "event_id": str(uuid.uuid4()),
            "source_service": "analytics-service",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "data": {
                "symbol": symbol,
                "sentiment": sentiment,
                "confidence": sentiment.get("confidence", 0.0)
            },
            "version": "1.0"
        }

        await self.producer.send_and_wait(
            "sentiment_updates",
            value=json.dumps(event).encode(),
            key=symbol.encode()
        )

    async def send_alert(self, user_id: int, alert_type: str, message: str):
        """ì•Œë¦¼ ì´ë²¤íŠ¸"""
        event = {
            "event_type": "user_alert",
            "event_id": str(uuid.uuid4()),
            "source_service": "backend-api",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "data": {
                "user_id": user_id,
                "alert_type": alert_type,
                "message": message
            },
            "version": "1.0"
        }

        # user_id ê¸°ë°˜ íŒŒí‹°ì…”ë‹
        await self.producer.send_and_wait(
            "alerts",
            value=json.dumps(event).encode(),
            key=str(user_id % 3).encode()  # 3ê°œ íŒŒí‹°ì…˜
        )

    async def send_user_event(self, user_id: int, event_type: str, data: Dict[str, Any]):
        """ì‚¬ìš©ì ì´ë²¤íŠ¸"""
        event = {
            "event_type": event_type,
            "event_id": str(uuid.uuid4()),
            "source_service": "backend-api",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "data": {
                "user_id": user_id,
                **data
            },
            "version": "1.0"
        }

        await self.producer.send_and_wait(
            "user_events",
            value=json.dumps(event).encode(),
            key=event_type.encode()
        )
```

**FastAPI í†µí•©**:

```python
# backend/main.py
from .services.kafka_event_producer import KafkaEventProducer

kafka_producer = None

@app.on_event("startup")
async def startup_kafka():
    global kafka_producer
    kafka_producer = KafkaEventProducer()
    await kafka_producer.start()
    logger.info("Kafka producer started")

@app.on_event("shutdown")
async def shutdown_kafka():
    if kafka_producer:
        await kafka_producer.stop()
        logger.info("Kafka producer stopped")

# ì—”ë“œí¬ì¸íŠ¸ì—ì„œ ì‚¬ìš©
@app.post("/api/v1/stocks/{symbol}")
async def update_stock(symbol: str, data: StockData):
    # ì£¼ê°€ ì—…ë°ì´íŠ¸
    await kafka_producer.send_stock_update(symbol, data.price, data.volume)
    # ...
```

---

### 4.3 Kafka ì»¨ìŠˆë¨¸ êµ¬í˜„

**íŒŒì¼**: `backend/services/kafka_event_consumer.py`

```python
from aiokafka import AIOKafkaConsumer
import json
from typing import Callable, Dict
import asyncio

class KafkaEventConsumer:
    def __init__(self, bootstrap_servers="kafka:9092", group_id="insitechart-group"):
        self.bootstrap_servers = bootstrap_servers
        self.group_id = group_id
        self.consumer = None
        self.handlers: Dict[str, list] = {}  # event_type â†’ [handlers]

    async def start(self):
        """ì»¨ìŠˆë¨¸ ì‹œì‘"""
        self.consumer = AIOKafkaConsumer(
            bootstrap_servers=self.bootstrap_servers,
            group_id=self.group_id,
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            session_timeout_ms=30000,
            heartbeat_interval_ms=10000
        )
        await self.consumer.start()

    async def stop(self):
        """ì»¨ìŠˆë¨¸ ì¢…ë£Œ"""
        if self.consumer:
            await self.consumer.stop()

    def register_handler(self, event_type: str, handler: Callable):
        """ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ë“±ë¡"""
        if event_type not in self.handlers:
            self.handlers[event_type] = []
        self.handlers[event_type].append(handler)

    async def listen(self, topics: list):
        """í† í”½ êµ¬ë… ë° ë©”ì‹œì§€ ì²˜ë¦¬"""
        await self.consumer.subscribe(topics)

        try:
            async for message in self.consumer:
                event = message.value
                event_type = event.get("event_type")

                # ë“±ë¡ëœ í•¸ë“¤ëŸ¬ ì‹¤í–‰
                if event_type in self.handlers:
                    for handler in self.handlers[event_type]:
                        try:
                            await handler(event)
                        except Exception as e:
                            logger.error(f"Handler error for {event_type}: {e}")
        except Exception as e:
            logger.error(f"Consumer error: {e}")
        finally:
            await self.consumer.stop()

# ì‚¬ìš© ì˜ˆ
async def handle_stock_update(event: dict):
    """ì£¼ê°€ ì—…ë°ì´íŠ¸ í•¸ë“¤ëŸ¬"""
    stock_data = event["data"]
    symbol = stock_data["symbol"]

    # ìºì‹œ ì—…ë°ì´íŠ¸
    await cache.set(f"stock:{symbol}", stock_data)

    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    await db.save_stock_price(symbol, stock_data["price"])

    # ì‚¬ìš©ìì—ê²Œ WebSocketìœ¼ë¡œ ì•Œë¦¼
    await websocket_manager.broadcast(f"stock_updates:{symbol}", event)

async def handle_sentiment_update(event: dict):
    """ì„¼í‹°ë¨¼íŠ¸ ì—…ë°ì´íŠ¸ í•¸ë“¤ëŸ¬"""
    sentiment_data = event["data"]
    symbol = sentiment_data["symbol"]

    # ìºì‹œ ì—…ë°ì´íŠ¸
    await cache.set(f"sentiment:{symbol}", sentiment_data)

    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    await db.save_sentiment(symbol, sentiment_data)
```

**FastAPI í†µí•©**:

```python
# backend/main.py
kafka_consumer = None

@app.on_event("startup")
async def startup_kafka_consumer():
    global kafka_consumer
    kafka_consumer = KafkaEventConsumer()
    await kafka_consumer.start()

    # í•¸ë“¤ëŸ¬ ë“±ë¡
    kafka_consumer.register_handler("stock_update", handle_stock_update)
    kafka_consumer.register_handler("sentiment_update", handle_sentiment_update)

    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ë¦¬ìŠ¤ë‹ ì‹œì‘
    asyncio.create_task(
        kafka_consumer.listen([
            "stock_updates",
            "sentiment_updates",
            "alerts",
            "user_events",
            "system_events"
        ])
    )
```

---

### 4.4 ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥ ë° ì¤‘ë³µ ì²˜ë¦¬

**Idempotent Consumer êµ¬í˜„**:

```python
class IdempotentEventProcessor:
    """ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ë³´ì¥"""

    def __init__(self, redis_client):
        self.redis = redis_client
        self.ttl = 86400  # 24ì‹œê°„

    async def process_event(self, event: dict, handler: Callable) -> bool:
        """
        ì´ë²¤íŠ¸ ì²˜ë¦¬ (ì¤‘ë³µ ì œê±°)
        Returns: True if processed, False if duplicate
        """
        event_id = event.get("event_id")

        # ì´ë¯¸ ì²˜ë¦¬ëœ ì´ë²¤íŠ¸ì¸ì§€ í™•ì¸
        key = f"processed_event:{event_id}"
        if await self.redis.exists(key):
            logger.warning(f"Duplicate event detected: {event_id}")
            return False

        try:
            # ì´ë²¤íŠ¸ ì²˜ë¦¬
            await handler(event)

            # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ (TTLê³¼ í•¨ê»˜ ì €ì¥)
            await self.redis.setex(key, self.ttl, "processed")
            return True
        except Exception as e:
            logger.error(f"Event processing failed: {e}")
            return False

# ì‚¬ìš©
processor = IdempotentEventProcessor(redis_client)

async def handle_stock_update_idempotent(event: dict):
    """ì¤‘ë³µ ì œê±°ë˜ëŠ” í•¸ë“¤ëŸ¬"""
    processed = await processor.process_event(
        event,
        handle_stock_update
    )
    if processed:
        logger.info(f"Event {event['event_id']} processed successfully")
```

**ìˆœì„œ ë³´ì¥ ì „ëµ**:

```python
class OrderedEventProcessor:
    """ì´ë²¤íŠ¸ ìˆœì„œ ë³´ì¥"""

    def __init__(self, redis_client):
        self.redis = redis_client
        self.queues = {}  # key â†’ queue

    async def process_ordered(self, partition_key: str, event: dict, handler: Callable):
        """
        íŒŒí‹°ì…˜ í‚¤ì— ë”°ë¼ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        ê°™ì€ íŒŒí‹°ì…˜ í‚¤ëŠ” í•­ìƒ ê°™ì€ ìˆœì„œë¡œ ì²˜ë¦¬ë¨
        """
        queue_key = f"event_queue:{partition_key}"

        # ì´ë²¤íŠ¸ë¥¼ íì— ì¶”ê°€
        await self.redis.rpush(queue_key, json.dumps(event))

        # ì´ íŒŒí‹°ì…˜ì˜ ì²˜ë¦¬ ì¤‘ ì—¬ë¶€ í™•ì¸
        processing_key = f"processing:{partition_key}"

        if await self.redis.get(processing_key):
            # ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì´ë©´ ëŒ€ê¸°
            return

        # ì²˜ë¦¬ ì‹œì‘ í‘œì‹œ
        await self.redis.setex(processing_key, 3600, "1")  # 1ì‹œê°„ TTL

        # íì˜ ëª¨ë“  ì´ë²¤íŠ¸ ì²˜ë¦¬
        while True:
            event_json = await self.redis.lpop(queue_key)
            if not event_json:
                break

            event = json.loads(event_json)
            await handler(event)

        # ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ
        await self.redis.delete(processing_key)
```

---

### 4.5 Kafka ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ

**Kafka ë©”íŠ¸ë¦­ ìˆ˜ì§‘**: `backend/monitoring/kafka_monitor.py`

```python
from prometheus_client import Counter, Histogram, Gauge
import time

# ë©”íŠ¸ë¦­ ì •ì˜
kafka_messages_produced = Counter(
    'kafka_messages_produced_total',
    'Total Kafka messages produced',
    ['topic']
)

kafka_messages_consumed = Counter(
    'kafka_messages_consumed_total',
    'Total Kafka messages consumed',
    ['topic', 'consumer_group']
)

kafka_consumer_lag = Gauge(
    'kafka_consumer_lag',
    'Consumer lag in messages',
    ['topic', 'partition', 'consumer_group']
)

kafka_processing_duration = Histogram(
    'kafka_message_processing_duration_seconds',
    'Time to process Kafka message',
    ['topic', 'event_type'],
    buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 5.0]
)

class KafkaMonitor:
    def __init__(self, admin_client):
        self.admin = admin_client

    async def monitor_consumer_lag(self):
        """ì»¨ìŠˆë¨¸ ë˜ê·¸ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                # ì»¨ìŠˆë¨¸ ê·¸ë£¹ì˜ ë˜ê·¸ ì •ë³´ ì¡°íšŒ
                partitions = await self.admin.fetch_consumer_offsets(
                    group_id="insitechart-group"
                )

                for (topic, partition), offset in partitions.items():
                    # ìµœì‹  ì˜¤í”„ì…‹ ì¡°íšŒ
                    latest = await self.admin.fetch_committed_offsets(
                        topic,
                        partitions=[partition]
                    )

                    lag = latest[partition].offset - offset.offset
                    kafka_consumer_lag.labels(
                        topic=topic,
                        partition=partition,
                        consumer_group="insitechart-group"
                    ).set(lag)

                    # ë˜ê·¸ê°€ ë†’ìœ¼ë©´ ì•Œë¦¼
                    if lag > 10000:
                        logger.warning(
                            f"High consumer lag detected: "
                            f"topic={topic}, partition={partition}, lag={lag}"
                        )

            except Exception as e:
                logger.error(f"Consumer lag monitoring error: {e}")

            await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ í™•ì¸
```

**Grafana ëŒ€ì‹œë³´ë“œ** (í–¥í›„):
```yaml
# monitoring/grafana/dashboards/kafka-dashboard.json
{
  "dashboard": {
    "title": "Kafka Cluster Monitoring",
    "panels": [
      {
        "title": "Messages Produced per Topic",
        "targets": [
          {
            "expr": "rate(kafka_messages_produced_total[1m])"
          }
        ]
      },
      {
        "title": "Consumer Lag",
        "targets": [
          {
            "expr": "kafka_consumer_lag"
          }
        ]
      },
      {
        "title": "Processing Duration",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, kafka_message_processing_duration_seconds)"
          }
        ]
      }
    ]
  }
}
```

---

## Phase 5: GDPR ìë™í™” ì‹œìŠ¤í…œ ì™„ì„±

**ëª©í‘œ**: GDPR ê·œì •ì„ ì™„ì „íˆ ìë™í™”í•˜ì—¬ ì¤€ìˆ˜
**ê¸°í•œ**: 3ì£¼
**í˜„ì¬ ìƒíƒœ**: ê¸°ë³¸ êµ¬ì¡°ë§Œ êµ¬í˜„ë¨

### 5.1 ë°ì´í„° ë³´ì¡´ ì •ì±… ìë™í™”

**êµ¬í˜„ íŒŒì¼**: `backend/services/gdpr_data_retention_service.py`

```python
from datetime import datetime, timedelta
from typing import List, Dict
import asyncio

class GDPRDataRetentionService:
    """GDPR ë°ì´í„° ë³´ì¡´ ì •ì±… ìë™í™”"""

    # ë°ì´í„°ë³„ ë³´ì¡´ ê¸°ê°„
    RETENTION_POLICIES = {
        "user_data": 365,  # 1ë…„
        "user_activity": 90,  # 3ê°œì›”
        "sentiment_data": 180,  # 6ê°œì›”
        "price_history": 730,  # 2ë…„
        "system_logs": 30,  # 30ì¼
        "error_logs": 90,  # 3ê°œì›”
    }

    def __init__(self, db_connection, logger):
        self.db = db_connection
        self.logger = logger

    async def cleanup_expired_data(self):
        """ë§Œë£Œëœ ë°ì´í„° ìë™ ì‚­ì œ"""
        for data_type, retention_days in self.RETENTION_POLICIES.items():
            cutoff_date = datetime.utcnow() - timedelta(days=retention_days)

            try:
                if data_type == "user_activity":
                    await self._cleanup_user_activity(cutoff_date)
                elif data_type == "sentiment_data":
                    await self._cleanup_sentiment_data(cutoff_date)
                elif data_type == "system_logs":
                    await self._cleanup_system_logs(cutoff_date)
                # ... ë‹¤ë¥¸ ë°ì´í„° íƒ€ì…

                self.logger.info(
                    f"Cleaned up {data_type} older than {cutoff_date}"
                )
            except Exception as e:
                self.logger.error(f"Cleanup error for {data_type}: {e}")
                # ì—ëŸ¬ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰

    async def _cleanup_user_activity(self, cutoff_date: datetime):
        """ì‚¬ìš©ì í™œë™ ë°ì´í„° ì‚­ì œ"""
        query = """
        DELETE FROM user_activity
        WHERE created_at < %s
        """
        await self.db.execute(query, (cutoff_date,))

        # ê°ì‚¬ ë¡œê·¸
        await self._log_deletion("user_activity", cutoff_date)

    async def _cleanup_sentiment_data(self, cutoff_date: datetime):
        """ì„¼í‹°ë¨¼íŠ¸ ë°ì´í„° ì‚­ì œ (íŠ¹íˆ ê°œì¸ ì •ë³´ í¬í•¨)"""
        query = """
        DELETE FROM sentiment_data
        WHERE timestamp < %s
        AND source IN ('REDDIT', 'TWITTER')  -- ì†Œì…œ ë¯¸ë””ì–´ ë°ì´í„°ë§Œ
        """
        await self.db.execute(query, (cutoff_date,))

        await self._log_deletion("sentiment_data", cutoff_date)

    async def _cleanup_system_logs(self, cutoff_date: datetime):
        """ì‹œìŠ¤í…œ ë¡œê·¸ ì‚­ì œ"""
        query = """
        DELETE FROM system_logs
        WHERE timestamp < %s
        """
        await self.db.execute(query, (cutoff_date,))

        await self._log_deletion("system_logs", cutoff_date)

    async def _log_deletion(self, data_type: str, cutoff_date: datetime):
        """ì‚­ì œ ì‘ì—… ê°ì‚¬ ë¡œê·¸"""
        query = """
        INSERT INTO gdpr_audit_log (action, data_type, cutoff_date, executed_at)
        VALUES (%s, %s, %s, %s)
        """
        await self.db.execute(
            query,
            ("automated_deletion", data_type, cutoff_date, datetime.utcnow())
        )

# ì •ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ (ë§¤ì¼ ìì •)
async def run_scheduled_cleanup():
    """ìŠ¤ì¼€ì¤„ëœ ì •ë¦¬ ì‘ì—…"""
    service = GDPRDataRetentionService(db, logger)

    # APScheduler ë˜ëŠ” Celery ì‚¬ìš©
    scheduler = BackgroundScheduler()
    scheduler.add_job(
        service.cleanup_expired_data,
        'cron',
        hour=0,  # ë§¤ì¼ ìì •
        minute=0
    )
    scheduler.start()
```

**ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì¶”ê°€**:

```sql
-- GDPR ê°ì‚¬ ë¡œê·¸ í…Œì´ë¸”
CREATE TABLE gdpr_audit_log (
    id SERIAL PRIMARY KEY,
    action VARCHAR(100),  -- 'automated_deletion', 'user_export', 'user_deletion'
    data_type VARCHAR(100),
    user_id INT REFERENCES users(id) ON DELETE SET NULL,
    cutoff_date TIMESTAMP,
    rows_affected INT DEFAULT 0,
    executed_at TIMESTAMP DEFAULT NOW(),
    completed_at TIMESTAMP,
    status VARCHAR(20),  -- 'pending', 'completed', 'failed'
    error_message TEXT
);

-- ë°ì´í„° ë³´ì¡´ ì •ì±… í…Œì´ë¸”
CREATE TABLE gdpr_retention_policies (
    id SERIAL PRIMARY KEY,
    data_type VARCHAR(100) UNIQUE,
    retention_days INT,
    description TEXT,
    last_cleanup TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- ì´ˆê¸° ì •ì±… ë°ì´í„°
INSERT INTO gdpr_retention_policies (data_type, retention_days, description) VALUES
('user_activity', 90, 'ì‚¬ìš©ì í™œë™ ë°ì´í„°'),
('sentiment_data', 180, 'ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ ë°ì´í„°'),
('price_history', 730, 'ì£¼ê°€ ì´ë ¥ ë°ì´í„°'),
('system_logs', 30, 'ì‹œìŠ¤í…œ ë¡œê·¸'),
('error_logs', 90, 'ì—ëŸ¬ ë¡œê·¸'),
('user_session', 90, 'ì‚¬ìš©ì ì„¸ì…˜');
```

---

### 5.2 ê°œì¸ì •ë³´ ìë™ ì‚­ì œ ê¸°ëŠ¥ (Right to be Forgotten)

**êµ¬í˜„ íŒŒì¼**: `backend/services/gdpr_user_deletion_service.py`

```python
from typing import Dict, List
from datetime import datetime
import asyncio

class GDPRUserDeletionService:
    """ì‚¬ìš©ì ì‚­ì œ ìš”ì²­ ìë™ ì²˜ë¦¬"""

    def __init__(self, db_connection, logger):
        self.db = db_connection
        self.logger = logger

    async def process_deletion_request(self, user_id: int) -> Dict:
        """ì‚¬ìš©ì ì‚­ì œ ìš”ì²­ ì²˜ë¦¬"""
        request_id = str(uuid.uuid4())

        try:
            # 1. ìš”ì²­ ê¸°ë¡
            await self._log_deletion_request(user_id, request_id)

            # 2. ê°œì¸ ì •ë³´ ì‚­ì œ
            await self._anonymize_user_data(user_id)

            # 3. ê´€ë ¨ ë°ì´í„° ì‚­ì œ
            await self._delete_user_data(user_id)

            # 4. ìµœì¢… ì‚¬ìš©ì ë ˆì½”ë“œ ì‚­ì œ
            await self._delete_user_account(user_id)

            # 5. ì™„ë£Œ ê¸°ë¡
            await self._log_completion(user_id, request_id, "success")

            return {
                "status": "success",
                "request_id": request_id,
                "message": "ì‚¬ìš©ì ë°ì´í„°ê°€ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"
            }

        except Exception as e:
            self.logger.error(f"Deletion error for user {user_id}: {e}")
            await self._log_completion(user_id, request_id, "failed", str(e))

            return {
                "status": "failed",
                "request_id": request_id,
                "error": str(e)
            }

    async def _anonymize_user_data(self, user_id: int):
        """ê°œì¸ ì •ë³´ ìµëª…í™”"""
        # ì‚¬ìš©ì í…Œì´ë¸”ì—ì„œ ê°œì¸ ì •ë³´ ì œê±°
        query = """
        UPDATE users
        SET
            email = CONCAT('deleted_', %s, '@deleted.local'),
            username = CONCAT('deleted_user_', %s),
            password_hash = NULL,
            is_active = FALSE
        WHERE id = %s
        """
        await self.db.execute(query, (user_id, user_id, user_id))

        # ê°œì¸ ì„¤ì • ì‚­ì œ
        await self.db.execute(
            "DELETE FROM user_notification_settings WHERE user_id = %s",
            (user_id,)
        )

    async def _delete_user_data(self, user_id: int):
        """ì‚¬ìš©ì ê´€ë ¨ ë°ì´í„° ì‚­ì œ"""
        tables_to_delete = [
            "watchlist_items",
            "search_history",
            "user_sessions",
            "user_activity",
            "user_behavior",
            "user_feedback",
            "api_keys"
        ]

        for table in tables_to_delete:
            query = f"DELETE FROM {table} WHERE user_id = %s"
            try:
                await self.db.execute(query, (user_id,))
                self.logger.info(f"Deleted records from {table} for user {user_id}")
            except Exception as e:
                self.logger.warning(
                    f"Could not delete from {table}: {e}"
                )

    async def _delete_user_account(self, user_id: int):
        """ìµœì¢… ì‚¬ìš©ì ë ˆì½”ë“œ ì‚­ì œ"""
        query = "DELETE FROM users WHERE id = %s"
        await self.db.execute(query, (user_id,))
        self.logger.info(f"User account {user_id} deleted")

    async def _log_deletion_request(self, user_id: int, request_id: str):
        """ì‚­ì œ ìš”ì²­ ë¡œê¹…"""
        query = """
        INSERT INTO gdpr_deletion_requests
        (user_id, request_id, status, requested_at)
        VALUES (%s, %s, %s, %s)
        """
        await self.db.execute(
            query,
            (user_id, request_id, "processing", datetime.utcnow())
        )

    async def _log_completion(
        self,
        user_id: int,
        request_id: str,
        status: str,
        error: str = None
    ):
        """ì‚­ì œ ì™„ë£Œ ë¡œê¹…"""
        query = """
        UPDATE gdpr_deletion_requests
        SET status = %s, completed_at = %s, error_message = %s
        WHERE request_id = %s
        """
        await self.db.execute(
            query,
            (status, datetime.utcnow(), error, request_id)
        )
```

**API ì—”ë“œí¬ì¸íŠ¸**:

```python
# backend/api/gdpr_routes.py

@app.post("/api/v1/gdpr/delete")
async def request_user_deletion(
    current_user: User = Depends(get_current_user),
    confirmation: str = Body(...)  # "DELETE_MY_DATA"
):
    """ì‚¬ìš©ì ìì‹ ì˜ ë°ì´í„° ì‚­ì œ ìš”ì²­"""

    if confirmation != "DELETE_MY_DATA":
        raise HTTPException(status_code=400, detail="Invalid confirmation")

    deletion_service = GDPRUserDeletionService(db, logger)
    result = await deletion_service.process_deletion_request(current_user.id)

    return {
        "status": "success",
        "message": "ì‚­ì œ ìš”ì²­ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
        "request_id": result["request_id"]
    }

@app.get("/api/v1/gdpr/deletion-status/{request_id}")
async def check_deletion_status(request_id: str):
    """ì‚­ì œ ìš”ì²­ ìƒíƒœ í™•ì¸"""
    query = """
    SELECT status, completed_at, error_message
    FROM gdpr_deletion_requests
    WHERE request_id = %s
    """
    result = await db.fetchone(query, (request_id,))

    if not result:
        raise HTTPException(status_code=404, detail="Request not found")

    return {
        "request_id": request_id,
        "status": result["status"],
        "completed_at": result["completed_at"],
        "error": result["error_message"]
    }
```

---

### 5.3 ë™ì˜ ê´€ë¦¬ ë° ì¶”ì  ì‹œìŠ¤í…œ

**êµ¬í˜„ íŒŒì¼**: `backend/services/gdpr_consent_management_service.py`

```python
from enum import Enum
from datetime import datetime
from typing import List, Dict

class ConsentType(str, Enum):
    ANALYTICS = "analytics"  # ë¶„ì„ ì¶”ì 
    MARKETING = "marketing"  # ë§ˆì¼€íŒ… í†µì‹ 
    DATA_SHARING = "data_sharing"  # ì œ3ì ë°ì´í„° ê³µìœ 
    COOKIES = "cookies"  # í•„ìˆ˜ ì¿ í‚¤ ì™¸
    PROFILING = "profiling"  # í”„ë¡œíŒŒì¼ë§

class GDPRConsentManagementService:
    """ë™ì˜ ê´€ë¦¬ ë° ì¶”ì """

    def __init__(self, db_connection):
        self.db = db_connection

    async def save_consent(
        self,
        user_id: int,
        consent_type: ConsentType,
        granted: bool,
        ip_address: str = None,
        user_agent: str = None
    ) -> Dict:
        """ë™ì˜ ì €ì¥"""

        query = """
        INSERT INTO user_consents
        (user_id, consent_type, granted, ip_address, user_agent, granted_at, version)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        ON CONFLICT (user_id, consent_type)
        DO UPDATE SET
            granted = EXCLUDED.granted,
            ip_address = EXCLUDED.ip_address,
            user_agent = EXCLUDED.user_agent,
            granted_at = EXCLUDED.granted_at,
            version = EXCLUDED.version + 1
        RETURNING id, version
        """

        result = await self.db.fetchone(
            query,
            (
                user_id,
                consent_type.value,
                granted,
                ip_address,
                user_agent,
                datetime.utcnow(),
                1  # ì´ˆê¸° ë²„ì „
            )
        )

        return {
            "consent_id": result["id"],
            "consent_type": consent_type,
            "granted": granted,
            "version": result["version"]
        }

    async def get_user_consents(self, user_id: int) -> List[Dict]:
        """ì‚¬ìš©ì ë™ì˜ ëª©ë¡"""
        query = """
        SELECT consent_type, granted, granted_at, version
        FROM user_consents
        WHERE user_id = %s
        ORDER BY granted_at DESC
        """

        consents = await self.db.fetchall(query, (user_id,))

        return [
            {
                "consent_type": c["consent_type"],
                "granted": c["granted"],
                "granted_at": c["granted_at"].isoformat(),
                "version": c["version"]
            }
            for c in consents
        ]

    async def revoke_all_consents(self, user_id: int):
        """ëª¨ë“  ë™ì˜ ì² íšŒ"""
        query = """
        UPDATE user_consents
        SET granted = FALSE, revoked_at = %s
        WHERE user_id = %s AND granted = TRUE
        """

        await self.db.execute(query, (datetime.utcnow(), user_id))

    async def check_consent(self, user_id: int, consent_type: ConsentType) -> bool:
        """íŠ¹ì • ë™ì˜ í™•ì¸"""
        query = """
        SELECT granted
        FROM user_consents
        WHERE user_id = %s AND consent_type = %s
        """

        result = await self.db.fetchone(query, (user_id, consent_type.value))

        return result["granted"] if result else False

# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/v1/gdpr/consents")
async def get_consents(current_user: User = Depends(get_current_user)):
    """ì‚¬ìš©ì ë™ì˜ ëª©ë¡ ì¡°íšŒ"""
    service = GDPRConsentManagementService(db)
    consents = await service.get_user_consents(current_user.id)
    return {"consents": consents}

@app.post("/api/v1/gdpr/consents")
async def update_consent(
    consent_type: ConsentType,
    granted: bool,
    current_user: User = Depends(get_current_user),
    request: Request
):
    """ë™ì˜ ì €ì¥"""
    service = GDPRConsentManagementService(db)

    result = await service.save_consent(
        user_id=current_user.id,
        consent_type=consent_type,
        granted=granted,
        ip_address=request.client.host,
        user_agent=request.headers.get("user-agent")
    )

    return result
```

---

### 5.4 ë°ì´í„° ì ‘ê·¼ ë¡œê¹… ë° ê°ì‚¬ ì¶”ì 

**êµ¬í˜„ íŒŒì¼**: `backend/services/gdpr_audit_trail_service.py`

```python
from enum import Enum
from datetime import datetime
from typing import Any, Dict

class DataAccessType(str, Enum):
    READ = "read"
    WRITE = "write"
    DELETE = "delete"
    EXPORT = "export"

class GDPRAuditTrailService:
    """ë°ì´í„° ì ‘ê·¼ ê°ì‹œ ë° ë¡œê¹…"""

    def __init__(self, db_connection):
        self.db = db_connection

    async def log_data_access(
        self,
        user_id: int,
        access_type: DataAccessType,
        data_type: str,
        description: str,
        ip_address: str = None,
        result: bool = True
    ):
        """ë°ì´í„° ì ‘ê·¼ ë¡œê¹…"""

        query = """
        INSERT INTO gdpr_audit_trail
        (user_id, access_type, data_type, description, ip_address, result, accessed_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s)
        """

        await self.db.execute(
            query,
            (
                user_id,
                access_type.value,
                data_type,
                description,
                ip_address,
                result,
                datetime.utcnow()
            )
        )

    async def get_audit_trail(
        self,
        user_id: int,
        start_date: datetime = None,
        end_date: datetime = None,
        limit: int = 100
    ) -> List[Dict]:
        """ê°ì‚¬ ê¸°ë¡ ì¡°íšŒ"""

        query = """
        SELECT * FROM gdpr_audit_trail
        WHERE user_id = %s
        """

        params = [user_id]

        if start_date:
            query += " AND accessed_at >= %s"
            params.append(start_date)

        if end_date:
            query += " AND accessed_at <= %s"
            params.append(end_date)

        query += " ORDER BY accessed_at DESC LIMIT %s"
        params.append(limit)

        records = await self.db.fetchall(query, params)

        return [
            {
                "id": r["id"],
                "access_type": r["access_type"],
                "data_type": r["data_type"],
                "description": r["description"],
                "accessed_at": r["accessed_at"].isoformat(),
                "ip_address": r["ip_address"],
                "result": r["result"]
            }
            for r in records
        ]

    async def detect_suspicious_activity(self, user_id: int) -> List[Dict]:
        """ì˜ì‹¬ í™œë™ ê°ì§€"""

        # ì˜ˆ: ì§§ì€ ì‹œê°„ì— ë§ì€ ë°ì´í„° ì ‘ê·¼
        query = """
        SELECT
            accessed_at,
            COUNT(*) as access_count,
            COUNT(DISTINCT data_type) as unique_data_types
        FROM gdpr_audit_trail
        WHERE user_id = %s
        AND accessed_at > NOW() - INTERVAL 1 hour
        GROUP BY HOUR(accessed_at)
        HAVING COUNT(*) > 50  -- 1ì‹œê°„ì— 50íšŒ ì´ìƒ
        """

        suspicious = await self.db.fetchall(query, (user_id,))

        return [
            {
                "timestamp": s["accessed_at"].isoformat(),
                "access_count": s["access_count"],
                "data_types": s["unique_data_types"],
                "severity": "HIGH" if s["access_count"] > 100 else "MEDIUM"
            }
            for s in suspicious
        ]

# ë¯¸ë“¤ì›¨ì–´ì—ì„œ ìë™ìœ¼ë¡œ ë¡œê¹…
audit_service = GDPRAuditTrailService(db)

@app.middleware("http")
async def log_data_access(request: Request, call_next):
    """ëª¨ë“  API ìš”ì²­ì˜ ë°ì´í„° ì ‘ê·¼ ê¸°ë¡"""

    response = await call_next(request)

    # ì‚¬ìš©ì ì¸ì¦ëœ ê²½ìš°ë§Œ ë¡œê¹…
    if hasattr(request.state, "user_id"):
        user_id = request.state.user_id

        # ì ‘ê·¼ ìœ í˜• íŒë‹¨
        access_type = DataAccessType.READ
        if request.method in ["POST", "PUT"]:
            access_type = DataAccessType.WRITE
        elif request.method == "DELETE":
            access_type = DataAccessType.DELETE

        # ë°ì´í„° íƒ€ì… íŒë‹¨
        path_parts = request.url.path.split("/")
        data_type = path_parts[-1] if path_parts else "unknown"

        # ë¡œê¹…
        await audit_service.log_data_access(
            user_id=user_id,
            access_type=access_type,
            data_type=data_type,
            description=f"{request.method} {request.url.path}",
            ip_address=request.client.host,
            result=response.status_code < 400
        )

    return response
```

---

### 5.5 GDPR ì¤€ìˆ˜ ë³´ê³ ì„œ ìë™ ìƒì„±

**êµ¬í˜„ íŒŒì¼**: `backend/services/gdpr_compliance_reporting_service.py`

```python
from datetime import datetime, timedelta
from typing import Dict
import json

class GDPRComplianceReportingService:
    """GDPR ì¤€ìˆ˜ ë³´ê³ ì„œ ìë™ ìƒì„±"""

    def __init__(self, db_connection):
        self.db = db_connection

    async def generate_compliance_report(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict:
        """ì „ì²´ GDPR ì¤€ìˆ˜ ë³´ê³ ì„œ"""

        report = {
            "report_date": datetime.utcnow().isoformat(),
            "period": {
                "start": start_date.isoformat(),
                "end": end_date.isoformat()
            },
            "sections": {}
        }

        # 1. ë°ì´í„° ì²˜ë¦¬ í™œë™
        report["sections"]["data_processing"] = \
            await self._get_data_processing_summary(start_date, end_date)

        # 2. ë°ì´í„° ì‚­ì œ
        report["sections"]["data_deletions"] = \
            await self._get_deletion_summary(start_date, end_date)

        # 3. ì‚¬ìš©ì ë™ì˜
        report["sections"]["consents"] = \
            await self._get_consent_summary(start_date, end_date)

        # 4. ë°ì´í„° ì´ë™ (data portability)
        report["sections"]["data_portability"] = \
            await self._get_data_export_summary(start_date, end_date)

        # 5. ë³´ì•ˆ ì‚¬ê±´
        report["sections"]["security_incidents"] = \
            await self._get_security_incidents(start_date, end_date)

        # 6. ê°ì‚¬ ì¶”ì 
        report["sections"]["audit_summary"] = \
            await self._get_audit_summary(start_date, end_date)

        return report

    async def _get_data_processing_summary(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict:
        """ë°ì´í„° ì²˜ë¦¬ í™œë™ ìš”ì•½"""

        query = """
        SELECT
            COUNT(DISTINCT user_id) as total_users,
            COUNT(*) as total_operations,
            COUNT(CASE WHEN access_type = 'read' THEN 1 END) as reads,
            COUNT(CASE WHEN access_type = 'write' THEN 1 END) as writes,
            COUNT(CASE WHEN access_type = 'delete' THEN 1 END) as deletes
        FROM gdpr_audit_trail
        WHERE accessed_at BETWEEN %s AND %s
        """

        result = await self.db.fetchone(query, (start_date, end_date))

        return {
            "total_users_processed": result["total_users"],
            "total_operations": result["total_operations"],
            "reads": result["reads"],
            "writes": result["writes"],
            "deletes": result["deletes"]
        }

    async def _get_deletion_summary(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict:
        """ì‚­ì œ í™œë™ ìš”ì•½"""

        query = """
        SELECT
            COUNT(*) as deletion_requests,
            COUNT(CASE WHEN status = 'success' THEN 1 END) as successful,
            COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed,
            SUM(CASE WHEN status = 'completed' THEN rows_affected ELSE 0 END) as rows_deleted
        FROM gdpr_deletion_requests
        WHERE requested_at BETWEEN %s AND %s
        """

        result = await self.db.fetchone(query, (start_date, end_date))

        return {
            "deletion_requests": result["deletion_requests"],
            "successful_deletions": result["successful"],
            "failed_deletions": result["failed"],
            "total_rows_deleted": result["rows_deleted"] or 0
        }

    async def _get_consent_summary(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict:
        """ë™ì˜ ê´€ë¦¬ ìš”ì•½"""

        query = """
        SELECT
            consent_type,
            COUNT(CASE WHEN granted THEN 1 END) as granted_count,
            COUNT(CASE WHEN NOT granted THEN 1 END) as withheld_count
        FROM user_consents
        WHERE granted_at BETWEEN %s AND %s
        GROUP BY consent_type
        """

        results = await self.db.fetchall(query, (start_date, end_date))

        consent_summary = {}
        for row in results:
            consent_summary[row["consent_type"]] = {
                "granted": row["granted_count"],
                "withheld": row["withheld_count"],
                "grant_rate": row["granted_count"] / (row["granted_count"] + row["withheld_count"]) * 100
            }

        return consent_summary

    async def _get_data_export_summary(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict:
        """ë°ì´í„° ì´ë™ì„± (Portability) ìš”ì•½"""

        query = """
        SELECT
            COUNT(*) as export_requests,
            COUNT(CASE WHEN status = 'success' THEN 1 END) as successful,
            AVG(EXTRACT(EPOCH FROM (completed_at - requested_at))) as avg_completion_time_seconds
        FROM gdpr_data_exports
        WHERE requested_at BETWEEN %s AND %s
        """

        result = await self.db.fetchone(query, (start_date, end_date))

        return {
            "export_requests": result["export_requests"],
            "successful_exports": result["successful"],
            "avg_completion_time_seconds": result["avg_completion_time_seconds"] or 0
        }

    async def _get_security_incidents(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict:
        """ë³´ì•ˆ ì‚¬ê±´ ìš”ì•½"""

        query = """
        SELECT
            severity,
            COUNT(*) as count
        FROM security_incidents
        WHERE detected_at BETWEEN %s AND %s
        GROUP BY severity
        """

        results = await self.db.fetchall(query, (start_date, end_date))

        incidents = {}
        for row in results:
            incidents[row["severity"]] = row["count"]

        return {
            "total_incidents": sum(incidents.values()),
            "by_severity": incidents
        }

    async def _get_audit_summary(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict:
        """ê°ì‚¬ í™œë™ ìš”ì•½"""

        query = """
        SELECT
            COUNT(*) as total_audits,
            COUNT(DISTINCT user_id) as audited_users,
            AVG(EXTRACT(EPOCH FROM accessed_at - accessed_at)) as avg_response_time
        FROM gdpr_audit_trail
        WHERE accessed_at BETWEEN %s AND %s
        """

        result = await self.db.fetchone(query, (start_date, end_date))

        return {
            "total_audit_entries": result["total_audits"],
            "users_with_activity": result["audited_users"],
            "audit_completeness": "100%"  # ëª¨ë“  ì ‘ê·¼ì´ ë¡œê¹…ë˜ëŠ” ê²½ìš°
        }

    async def export_report_as_pdf(self, report: Dict) -> bytes:
        """ë³´ê³ ì„œë¥¼ PDFë¡œ ë‚´ë³´ë‚´ê¸°"""
        # ReportLab ë˜ëŠ” PyPDF2 ì‚¬ìš©
        from reportlab.lib.pagesizes import letter
        from reportlab.lib.styles import getSampleStyleSheet
        from reportlab.platypus import SimpleDocTemplate, Paragraph, PageBreak

        # PDF ìƒì„± ë¡œì§...
        # (ìì„¸í•œ êµ¬í˜„ì€ ë³„ë„)
        pass

# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/api/v1/gdpr/compliance-report")
async def get_compliance_report(
    start_date: datetime = Query(...),
    end_date: datetime = Query(...),
    format: str = Query("json", regex="^(json|pdf)$")
):
    """GDPR ì¤€ìˆ˜ ë³´ê³ ì„œ ì¡°íšŒ"""

    service = GDPRComplianceReportingService(db)
    report = await service.generate_compliance_report(start_date, end_date)

    if format == "pdf":
        pdf_bytes = await service.export_report_as_pdf(report)
        return StreamingResponse(
            io.BytesIO(pdf_bytes),
            media_type="application/pdf",
            headers={"Content-Disposition": "attachment; filename=gdpr_report.pdf"}
        )
    else:
        return report
```

---

## ì‘ì—… ì˜ì¡´ì„±

```
Phase 1: ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™”
    â”œâ”€ 1.1: WebSocket ì¬ì—°ê²° (ì„ í–‰ í•„ìˆ˜ ì—†ìŒ)
    â”œâ”€ 1.2: ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ (1.1 ì™„ë£Œ í›„)
    â”œâ”€ 1.3: Redis Pub/Sub (1.1, 1.2 ì™„ë£Œ í›„)
    â”œâ”€ 1.4: ë™ì‹œì„± ì²˜ë¦¬ (1.3 ì™„ë£Œ í›„)
    â””â”€ 1.5: ì•Œë¦¼ í…œí”Œë¦¿ (1.1-1.4 ì™„ë£Œ í›„)

Phase 2: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬
    â”œâ”€ 2.1: ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ (ì„ í–‰ í•„ìˆ˜ ì—†ìŒ, Phase 1ê³¼ ë³‘ë ¬)
    â”œâ”€ 2.2: ë¶„ì„ ì„œë¹„ìŠ¤ (2.1 ì™„ë£Œ í›„)
    â”œâ”€ 2.3: API ê²Œì´íŠ¸ì›¨ì´ (2.1, 2.2 ì™„ë£Œ í›„)
    â”œâ”€ 2.4: ì„œë¹„ìŠ¤ ê°„ í†µì‹  (2.3 ì™„ë£Œ í›„, Phase 4ì™€ ì—°ê³„)
    â””â”€ 2.5: ë°°í¬ ì„¤ì • (2.1-2.3 ì™„ë£Œ í›„)

Phase 3: CI/CD íŒŒì´í”„ë¼ì¸
    â”œâ”€ 3.1: GitHub Actions ì›Œí¬í”Œë¡œìš° (ì„ í–‰ í•„ìˆ˜ ì—†ìŒ)
    â”œâ”€ 3.2: í…ŒìŠ¤íŠ¸ í™˜ê²½ (3.1 ì™„ë£Œ í›„)
    â”œâ”€ 3.3: ë¹Œë“œ íŒŒì´í”„ë¼ì¸ (3.1, 3.2 ì™„ë£Œ í›„)
    â”œâ”€ 3.4: ë°°í¬ ì „ëµ (3.3 ì™„ë£Œ í›„)
    â””â”€ 3.5: ëª¨ë‹ˆí„°ë§ (3.4 ì™„ë£Œ í›„)

Phase 4: Kafka ì´ë²¤íŠ¸ ë²„ìŠ¤
    â”œâ”€ 4.1: Kafka í´ëŸ¬ìŠ¤í„° ì„¤ì • (ì„ í–‰ í•„ìˆ˜: Phase 2 ì™„ë£Œ)
    â”œâ”€ 4.2: í”„ë¡œë“€ì„œ (4.1 ì™„ë£Œ í›„)
    â”œâ”€ 4.3: ì»¨ìŠˆë¨¸ (4.2 ì™„ë£Œ í›„)
    â”œâ”€ 4.4: ìˆœì„œ ë³´ì¥ (4.3 ì™„ë£Œ í›„)
    â””â”€ 4.5: ëª¨ë‹ˆí„°ë§ (4.4 ì™„ë£Œ í›„)

Phase 5: GDPR ìë™í™”
    â”œâ”€ 5.1: ë°ì´í„° ë³´ì¡´ ì •ì±… (ì„ í–‰ í•„ìˆ˜ ì—†ìŒ)
    â”œâ”€ 5.2: ì‚¬ìš©ì ì‚­ì œ (5.1 ì™„ë£Œ í›„)
    â”œâ”€ 5.3: ë™ì˜ ê´€ë¦¬ (5.2 ì™„ë£Œ í›„)
    â”œâ”€ 5.4: ê°ì‚¬ ì¶”ì  (5.3 ì™„ë£Œ í›„)
    â””â”€ 5.5: ë³´ê³ ì„œ ìƒì„± (5.4 ì™„ë£Œ í›„)
```

---

## ìœ„í—˜ ë¶„ì„

### ğŸ”´ ë†’ì€ ìœ„í—˜ (High Risk)

| ìœ„í—˜ | ì˜í–¥ | ì™„í™” ì „ëµ |
|------|------|---------|
| **WebSocket ëŒ€ê·œëª¨ ë™ì‹œ ì—°ê²° ì‹¤íŒ¨** | ì‹¤ì‹œê°„ ë°ì´í„° ì„œë¹„ìŠ¤ ì¤‘ë‹¨ | Phase 1.1ì—ì„œ ì¶©ë¶„í•œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ (5000+ ë™ì‹œ) |
| **ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë¶„ë¦¬ ì‹œ ë°ì´í„° ë¶ˆì¼ì¹˜** | ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì˜¤ë¥˜ | íŠ¸ëœì­ì…˜ ë¡œê·¸, ê°ì‹œ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ |
| **Kafka ë©”ì‹œì§€ ì†ì‹¤** | ì¤‘ìš” ë°ì´í„° ìœ ì‹¤ | acks='all', ë³µì œë³¸ ì„¤ì •, ë°ë“œë ˆí„° í |
| **CI/CD ìë™í™” ì˜¤ë¥˜** | ì˜ëª»ëœ ë°°í¬ | ìŠ¤í…Œì´ì§• í™˜ê²½ì—ì„œ ì² ì €í•œ í…ŒìŠ¤íŠ¸ |

### ğŸŸ  ì¤‘ê°„ ìœ„í—˜ (Medium Risk)

| ìœ„í—˜ | ì˜í–¥ | ì™„í™” ì „ëµ |
|------|------|---------|
| **GDPR ì¤€ìˆ˜ ë¶ˆì™„ì „** | ë²•ì  ì²˜ë²Œ | ë²•ë¥ íŒ€ ê²€í† , ì •ê¸° ê°ì‹œ |
| **ì„±ëŠ¥ ì €í•˜** | ì‚¬ìš©ì ê²½í—˜ ì•…í™” | ë¶€í•˜ í…ŒìŠ¤íŠ¸, ëª¨ë‹ˆí„°ë§ |
| **ë³´ì•ˆ ì·¨ì•½ì ** | ë°ì´í„° ìœ ì¶œ | ë³´ì•ˆ ìŠ¤ìºë‹, ì¹¨íˆ¬ í…ŒìŠ¤íŠ¸ |

### ğŸŸ¡ ë‚®ì€ ìœ„í—˜ (Low Risk)

| ìœ„í—˜ | ì˜í–¥ | ì™„í™” ì „ëµ |
|------|------|---------|
| **ê°œë°œì í•™ìŠµ ê³¡ì„ ** | ì´ˆê¸° ì§„í–‰ ì§€ì—° | ë¬¸ì„œí™”, íŒ€ íŠ¸ë ˆì´ë‹ |
| **ì˜ì¡´ì„± ë²„ì „ í˜¸í™˜ì„±** | ë¹Œë“œ ì‹¤íŒ¨ | ì •ê¸° ì˜ì¡´ì„± ì—…ë°ì´íŠ¸ |

---

## ì„±ê³µ ê¸°ì¤€

### Phase 1 ì„±ê³µ ê¸°ì¤€
- âœ… WebSocket ìµœì†Œ 1000ê°œ ë™ì‹œ ì—°ê²° ìœ ì§€
- âœ… ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë° ì§€ì—° ì‹œê°„ < 1ì´ˆ
- âœ… ë©”ì‹œì§€ ì†ì‹¤ìœ¨ = 0%
- âœ… ì‹œìŠ¤í…œ ê°€ìš©ì„± â‰¥ 99.5%
- âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ â‰¥ 80%

### Phase 2 ì„±ê³µ ê¸°ì¤€
- âœ… 4ê°œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ë…ë¦½ ë°°í¬ ê°€ëŠ¥
- âœ… ì„œë¹„ìŠ¤ë³„ ë…ë¦½ ìŠ¤ì¼€ì¼ë§ ê°€ëŠ¥
- âœ… API ê²Œì´íŠ¸ì›¨ì´ ìš”ì²­ ì²˜ë¦¬ < 100ms
- âœ… ì„œí‚· ë¸Œë ˆì´ì»¤ ì •ìƒ ì‘ë™ (ì¥ì•  ê°ì§€ < 30ì´ˆ)

### Phase 3 ì„±ê³µ ê¸°ì¤€
- âœ… ëª¨ë“  PRì— ëŒ€í•´ CI/CD ìë™ ì‹¤í–‰
- âœ… ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ â‰¥ 80%
- âœ… ë°°í¬ ì‹œê°„ < 5ë¶„
- âœ… ìŠ¤í…Œì´ì§• í™˜ê²½ì—ì„œ ê²€ì¦ < 2ì‹œê°„

### Phase 4 ì„±ê³µ ê¸°ì¤€
- âœ… Kafka í† í”½ ìƒì„± ë° ê²€ì¦ ì™„ë£Œ
- âœ… í”„ë¡œë“€ì„œ/ì»¨ìŠˆë¨¸ throughput > 1000 msg/sec
- âœ… ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥ (ê°™ì€ íŒŒí‹°ì…˜ ë‚´)
- âœ… ì»¨ìŠˆë¨¸ ë˜ê·¸ < 1000 ë©”ì‹œì§€

### Phase 5 ì„±ê³µ ê¸°ì¤€
- âœ… ìë™ ë°ì´í„° ì •ë¦¬ ì •ì±… ì‹¤í–‰ í™•ì¸
- âœ… ì‚¬ìš©ì ì‚­ì œ ìš”ì²­ ì²˜ë¦¬ < 24ì‹œê°„
- âœ… ë™ì˜ ê´€ë¦¬ API ì •ìƒ ì‘ë™
- âœ… ê°ì‚¬ ë¡œê·¸ ëˆ„ë½ë¥  = 0%
- âœ… GDPR ì¤€ìˆ˜ ë³´ê³ ì„œ ìë™ ìƒì„± ê°€ëŠ¥

---

## ë‹¤ìŒ ë‹¨ê³„

1. **ì¦‰ì‹œ** (ì´ë²ˆ ì£¼):
   - ì‘ì—… ëª©ë¡ ìŠ¹ì¸
   - íŒ€ ì—­í•  ë°°ì¹˜
   - Phase 1 ìƒì„¸ ì¼ì • ìˆ˜ë¦½

2. **ì´ë²ˆ ì£¼** (1ì£¼):
   - Phase 1.1 (WebSocket) ê°œë°œ ì‹œì‘
   - Phase 2.1 (ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤) ê°œë°œ ì‹œì‘
   - Phase 3.1 (CI/CD) ê°œë°œ ì‹œì‘

3. **ë‹¤ìŒ ì£¼** (2ì£¼):
   - Phase 1 ì™„ë£Œ
   - Phase 2 ì‹œì‘
   - Phase 3 ì™„ë£Œ

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-12-11
**ë‹´ë‹¹ì**: Development Team
**ìƒíƒœ**: ì§„í–‰ ì¤‘

